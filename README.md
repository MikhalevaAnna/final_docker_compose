## Создание пайплайна для обработки и анализа данных о продажах

**Используя стек технологий: PostgreSQL, ClickHouse, Apache Airflow и PySpark.**

Пайплайн должен выполнять генерацию реалистичных данных о продажах, их обработку, очистку, загрузку в базы данных, а также выполнение аналитических операций.

## Пайплайн выполняет следующие задачи:

1) **DAG** `main.py` запускается в 12:45 по Москве, каждый вторник.
2) Происходит генерация правдоподобных данных о продажах.
   Файл `sales_data.csv` содержит 1 миллион записей за последний год, он хранится в `Docker` контейнере, в папке `sample_data`.
   Структура csv-файла:
      - `sale_id` (уникальный идентификатор продажи).
      - `customer_id` (идентификатор клиента).
      - `product_id` (идентификатор продукта).
      - `quantity` (количество купленных товаров).
      - `sale_date` (дата продажи).
      - `sale_amount` (сумма продажи, рассчитывается как количество товаров * случайная цена товара).
      - `region` (регион клиента, один из: North, South, East, West).
3) Выполняется очистка в **PySpark** от дубликатов и данные приводятся к нужным форматам для дальнейшей работы.
   Подготовленные данные загружаются в таблицу `sales_data` **PostgreSQL**.
4) В **PostgreSQL** выполняется следующая агрегация данных:
    - Подсчитывается общее количество продаж (`total_quantity`) и сумма продаж (`total_sale_amount`) для каждого региона и каждого продукта.
    - Рассчитывается средний чек (`average_sale_amount`) по регионам и продуктам.
    Агрегированные данные сохраняются в отдельную таблицу `sales_data_agg` в **PostgreSQL**.
5) Выполняется миграция агрегированных данных из таблицы `sales_data_agg` **PostgreSQL** в таблицу `sales_data_agg` **ClickHouse**, с добавлением поля `update_date`, для отслеживания даты обновления.
   
Все операции по созданию таблиц, обработке, миграции данных происходят внутри **Airflow**.

## Просмотр данных через DBeaver
В **DBeaver**, после успешной работы пайплайна, можно посмотреть полученные таблицы `sales_data` и `sales_data_agg` в **PostgreSQL** и `sales_data_agg` в **ClickHouse**.

### Для подключение к **PostgreSQL** используются следующие параметры:
```
    Хост: localhost
    Порт: 5432
    База данных: test
    Пользователь: user
    Пароль: password
```
### Для подключение к **ClickHouse** используются следующие параметры:
```
    Хост: localhost
    Порт: 8123
```
### Команды для запуска проекта:
```bash
    git clone https://github.com/MikhalevaAnna/pipeline_sales_data.git
    cd final_docker_compose
    docker build -t airflow-with-java .
    docker-compose up --build
```
    
- Далее идем по адресу - **http://localhost:8080**
- Логин и пароль - **airflow**.

